{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_movie_page_data(url):\n",
    "    '''\n",
    "    function to scrape each movie page from the url list\n",
    "    and return the html text of the page\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    ## make sure we connected\n",
    "    if response.status_code != requests.codes.ok:\n",
    "        print i, 'Error: request.get(url) Status NOT 200'\n",
    "        return None\n",
    "    page = response.text\n",
    "    return page\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_urls_from_imdb_archive_page():\n",
    "    '''\n",
    "    function to get the movie urls of the top 10,000 movies from the \n",
    "    imdb archive pages. The pages have about 250 movies on each page \n",
    "    and go to 40 pages total. \n",
    "    '''\n",
    "    ## 351 is the value present at the first page, and so that is what we start with. \n",
    "    list_position = 351\n",
    "    ## url_list will contain the links for the individual movies\n",
    "    url_list = []\n",
    "    for i in range(40):\n",
    "        ## the base url does not change, but the list_position does in increments of 250, \n",
    "        ## as mentioned in the doc string\n",
    "        url = 'http://www.imdb.com/list/ls057823854/?start={}&view=compact&sort=release_date_us:desc'.format(str(list_position))\n",
    "        print url\n",
    "        response = requests.get(url)\n",
    "        page = response.text\n",
    "        ## xml is necessary because the html5 parser cuts off the list pages.\n",
    "        soup = BeautifulSoup(page, 'xml')\n",
    "        ## get list of titles of the movies\n",
    "        title_list = soup.find_all(class_='title')\n",
    "        count = 0\n",
    "        ## make urls out of the titles by combining them with the \n",
    "        ## titles\n",
    "        for title in title_list:\n",
    "            while count < len(title_list):\n",
    "                a = title_list[count].contents\n",
    "                href = a[0].get('href')\n",
    "                imdb = 'http://www.imdb.com'\n",
    "                movie_url = str(imdb + href)\n",
    "                url_list.append(movie_url)\n",
    "                count += 1\n",
    "        ## add 250 so next pass of the loop goes to the next page\n",
    "        list_position += 250\n",
    "        return url_list\n",
    "                                                                                                         \n",
    "                                                                                                        \n",
    "                                                                                                         \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.imdb.com/list/ls057823854/?start=351&view=compact&sort=release_date_us:desc\n"
     ]
    }
   ],
   "source": [
    "url_list = scrape_urls_from_imdb_archive_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_movie_page_data(url):\n",
    "    '''\n",
    "    function to scrape each movie page from the url list\n",
    "    and return the html text of the page\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    ## make sure we connected\n",
    "    if response.status_code != requests.codes.ok:\n",
    "        print i, 'Error: request.get(url) Status NOT 200'\n",
    "        return None\n",
    "    page = response.text\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_release_date(soup):\n",
    "    '''\n",
    "    Function to get the release date of the movie by trying to find\n",
    "    it in numerous places on the page\n",
    "    '''\n",
    "    try:\n",
    "        if soup('h4')[20].contents == [u'Release Date:']:\n",
    "            date_string = str(soup('h4')[20].next.next)\n",
    "        elif soup('h4')[19].contents == [u'Release Date:']:\n",
    "            date_string = str(soup('h4')[19].next.next)\n",
    "        elif soup('h4')[17].contents == [u'Release Date:']:\n",
    "            date_string = str(soup('h4')[17].next.next)    \n",
    "        else:\n",
    "            date_string = str(soup('h4')[18].next.next)\n",
    "        date_split = date_string.split( ' ')\n",
    "        date = date_split[1] + ' ' + date_split[2] + ' ' + date_split[3]\n",
    "    except :\n",
    "        date = 'Error' \n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_genres(soup):\n",
    "    '''\n",
    "    Function to get the genres of the movie, and append\n",
    "    them to a list, which is returned.\n",
    "    '''\n",
    "    genre_list = []\n",
    "    genre_tags = soup('span', itemprop=\"genre\")\n",
    "    for item in genre_tags:\n",
    "        genre = str(item.string)\n",
    "        genre_list.append(genre)\n",
    "    return genre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_box_office_html(soup):\n",
    "    '''\n",
    "    Function to scrape the box office info page and get \n",
    "    the html text\n",
    "    '''\n",
    "    ## genereic url extension for movie's box office page\n",
    "    box_office_url_extension = 'business?ref_=tt_dt_bus'\n",
    "    url = str(soup(property=\"og:url\")[0]).split ('\"')[1]\n",
    "    box_office_url = url + box_office_url_extension\n",
    "    ## get box office info html text\n",
    "    response = requests.get(box_office_url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'xml')\n",
    "    ## make sure we get response\n",
    "    if response.status_code != requests.codes.ok:\n",
    "        print i, 'Error: request.get(url) Status NOT 200'\n",
    "        return None\n",
    "    else:\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_opening_weekend(box_soup):\n",
    "    '''\n",
    "    Function to try and find opening weekend dollar amount.\n",
    "    '''\n",
    "    try:\n",
    "        if box_soup(text='Weekend Gross'):\n",
    "            if '$' in box_soup(text='Weekend Gross')[0].next:\n",
    "                opening_string = str(box_soup(text='Weekend Gross')[0].next)\n",
    "                opening_weekend = int(re.sub(\"[^0-9]\", \"\", opening_string))\n",
    "            else: opening_weekend = 'N/A'\n",
    "        else: opening_weekend = 'N/A'\n",
    "    except UnicodeEncodeError:\n",
    "        opening_weekend = 'Error'\n",
    "    return opening_weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_opening_weekend_screens(box_soup):\n",
    "    '''\n",
    "    Function to get the ammount of screens the movie opened on\n",
    "    in its opening weekend and return that ammount.\n",
    "    '''\n",
    "    try:\n",
    "        screen_string = str(box_soup('h5')[15].next.next.next.next.next.next.next.next)\n",
    "        opening_weekend_screens = re.sub(r'\\W+', '', screen_string)\n",
    "    except :\n",
    "        opening_weekend_screens = 'Error'\n",
    "    return opening_weekend_screens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_movie_info(url_list):\n",
    "    '''\n",
    "    Function to scrape the title, release date, genres, and box office info(opening dollars)\n",
    "    and screens released) for each movie.\n",
    "    '''\n",
    "    movie_info = defaultdict(dict)\n",
    "    for url in url_list:\n",
    "        page = scrape_movie_page_data(url)\n",
    "        soup = BeautifulSoup(page, 'xml')\n",
    "        ##check if it's a tv movie (we do not want those)\n",
    "        tv_movie_tag = str(soup(class_=\"infobar\")[0].next)\n",
    "        try:\n",
    "            if re.sub(r'\\W+', '', tv_movie_tag) != 'TVMovie' and re.sub(r'\\W+', '', tv_movie_tag) != 'TVSpecial':\n",
    "                ## get title\n",
    "                head = soup(class_='header')\n",
    "                sp = head[0](class_=\"itemprop\")\n",
    "                title = str(sp[0].string)\n",
    "        except UnicodeEncodeError:\n",
    "            continue\n",
    "        ## get and enter release date entry\n",
    "        date = get_release_date(soup) \n",
    "        movie_info[title]['date']=date\n",
    "        ## get genres   \n",
    "        genre_list = get_genres(soup)\n",
    "        movie_info[title]['genre']=genre_list\n",
    "        ## go to box office info page to scrape that data\n",
    "        if get_box_office_html(soup):\n",
    "            box_soup = get_box_office_html(soup)\n",
    "        else:\n",
    "            opening_weekend = 'Error'\n",
    "            screens = 'Error'    \n",
    "        ## get opening weekend amount\n",
    "        opening_weekend = get_opening_weekend(box_soup)\n",
    "        movie_info[title]['opening']=opening_weekend\n",
    "        ## get amount of screens movie opened on \n",
    "        screens = get_opening_weekend_screens(box_soup)\n",
    "        movie_info[title]['screens']=screens\n",
    "    return movie_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_list = ['http://www.imdb.com/title/tt0241527/','http://www.imdb.com/title/tt0241527/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'dict'>, {\"Harry Potter and the Sorcerer's Stone\": {'date': '16 November 2001', 'genre': ['Adventure', 'Family', 'Fantasy'], 'screens': '3672Screens', 'opening': 38455}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_movie_info(url_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
